{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "### DSI7 Capstone Project\n",
    "---\n",
    "# Support Notebook for: Machine Learning Model for Breast Cancer Survival Prediction using Gene Expression Profiles \n",
    "\n",
    "\n",
    "<img src=\"https://biox.stanford.edu/sites/g/files/sbiybj7941/f/rna_polymerase_highlight_banner.png\" style=\"height: 250px; width: 1000px\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notbook is supporting notebook that includes all functions used in the capstone project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yellowbrick as yb\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Statistics, EDA, metrics libraries\n",
    "from scipy.stats import normaltest, skew\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "\n",
    "# Modeling libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "\n",
    "from IPython.display import set_matplotlib_formats \n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Pallets used for visualizations\n",
    "color= \"Spectral\"\n",
    "color_plt = ListedColormap(sns.color_palette(color).as_hex())\n",
    "color_hist = 'teal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a dataframe and transforms it into a standard form after dropping nun_numirical columns\n",
    "def to_standard (df):\n",
    "    \n",
    "    num_df = df[df.select_dtypes(include = np.number).columns.tolist()]\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    std = ss.fit_transform(num_df)\n",
    "    \n",
    "    std_df = pd.DataFrame(std, index = num_df.index, columns = num_df.columns)\n",
    "    return std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts x to int weither x is a string or a Series\n",
    "def to_int(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return int(x)\n",
    "        else:\n",
    "            return x.astype(int)\n",
    "    except ValueError:\n",
    "        return x\n",
    "\n",
    "# Function that converts x to float weither x is a string or a Series\n",
    "def to_float(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return float(x)\n",
    "        else:\n",
    "            return x.astype(float)\n",
    "    except ValueError:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a dataframe and plots histograms for all columns \n",
    "def subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels, big_title_name):\n",
    "    \n",
    "    nrows = int(np.ceil(len(list_of_columns)/3)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(ncols=3,nrows=nrows, figsize=(15, 10)) # You'll want to specify your figsize\n",
    "    fig.suptitle(big_title_name, fontsize=15)\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].hist(dataframe[column].dropna(), color= color_hist ) # feel free to add more settings\n",
    "        #ax[i].set_xlabel(list_of_xlabels[i])\n",
    "        ax[i].set_ylabel('Frequency')\n",
    "        ax[i].set_title(list_of_titles[i]) # Set titles, labels, etc here for each subplot    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a dataframe and plots barplot for all columns \n",
    "def subplot_bargraph(dataframe, list_of_columns, list_of_titles, list_of_xlabels, big_title_name):\n",
    "    \n",
    "    nrows = int(np.ceil(len(list_of_columns)/3)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(ncols=3,nrows=nrows, figsize=(15, 10)) # You'll want to specify your figsize\n",
    "    fig.suptitle(big_title_name, fontsize=20)\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        sns.countplot(dataframe[column].dropna(), color= color_hist, ax=ax[i], hue=dataframe['eventdeath']) # feel free to add more settings\n",
    "        #ax[i].set_xlabel(list_of_xlabels[i])\n",
    "        ax[i].set_xlabel('')\n",
    "        ax[i].set_ylabel('Frequency')\n",
    "        ax[i].set_title(list_of_titles[i]) # Set titles, labels, etc here for each subplot    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(model, kfold, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #metrics\n",
    "    results = cross_val_score(model, X_train, y_train, cv = kfold)\n",
    "    print(\"CV scores: \", results); print(\"CV Standard Deviation: \", results.std()); print();\n",
    "    print('CV Mean score: ', results.mean()); \n",
    "    print('Train score:   ', model.score(X_train, y_train))\n",
    "    print('Test score:    ', model.score(X_test, y_test))\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    # CODE HERE PLEASE\n",
    "    print()\n",
    "    print('Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "    print('Classification Report:  ')\n",
    "    print(classification_report(y_test, pred))\n",
    "    train_score =  model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    test_pred = model.predict(X_test)\n",
    "    return test_pred, test_score, results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_classifiers (X_train, X_test, y_train, y_test, kfold):\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "    \n",
    "    # Scaling \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ######################################################################################################  K Neighbors Classifier model\n",
    "    \n",
    "    params = {\n",
    "    \"n_neighbors\" : [5,15,25,30,35,40, 100],\n",
    "    \"weights\" : [\"uniform\" , \"distance\"]\n",
    "    }\n",
    "    print(); print(BOLD + 'K Neighbors Classifier Model:' + END)\n",
    "    knn= GridSearchCV(KNeighborsClassifier(), params, n_jobs=-1, cv=4)\n",
    "    knn_pred, knn_test, knn_train = model_metrics(knn, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    ###################################################################################################### Logistic Regression\n",
    "    params = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": np.logspace(-2,4,100)\n",
    "    }\n",
    "    print(); print(BOLD + 'Logistic Regression Model:' + END)\n",
    "    logistic_regression = GridSearchCV(LogisticRegression(random_state=42), params, n_jobs=-1, cv=4)\n",
    "    lg_pred, lg_test, lg_train = model_metrics(logistic_regression, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    ###################################################################################################### Decision Tree\n",
    "    \n",
    "    print(); print(BOLD + 'Decision Tree Classifier Model:' + END)\n",
    "    decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "    dt_pred, dt_test, dt_train = model_metrics(decision_tree, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    ###################################################################################################### Random Forest Classifier\n",
    "    \n",
    "    print(); print(BOLD + 'Random Forest Classifier Model:' + END)\n",
    "    random_forest = RandomForestClassifier(random_state=42)\n",
    "    rf_pred, rf_test, rf_train = model_metrics(random_forest, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    ###################################################################################################### Extra Trees Classifier\n",
    "   \n",
    "    print(); print(BOLD + 'Extra Trees Classifier Model:' + END)\n",
    "    extra_trees = ExtraTreesClassifier(random_state=42)\n",
    "    et_pred, et_test, et_train = model_metrics(extra_trees, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    ###################################################################################################### AdaBoost Classifier\n",
    "    \n",
    "    print(); print(BOLD + 'AdaBoost Classifier Model:' + END)\n",
    "    ada_boost = AdaBoostClassifier(random_state=42)\n",
    "    ab_pred, ab_test, ab_train = model_metrics(ada_boost, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    ###################################################################################################### SVC Classifier\n",
    "    \n",
    "    print(); print(BOLD + 'SVC Classifier Model:' + END)\n",
    "    svc = SVC(random_state=42)\n",
    "    svc_pred, svc_test, svc_train = model_metrics(svc, kfold, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(15,6))\n",
    "\n",
    "    \n",
    "    #bar chart of accuracy scores\n",
    "    inds = range(1,8)\n",
    "    labels = [\"KNN\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\",'Extra Trees', 'AdaBoost', 'SVC' ]\n",
    "    scores_all = [knn_train, lg_train, dt_train, rf_train, et_train, ab_train, svc_train]\n",
    "    scores_predictive = [knn_test, lg_test, dt_test, rf_test, et_test, ab_test, svc_test]\n",
    "    \n",
    "    ax1.bar(inds, scores_all, color=sns.color_palette(color)[5], alpha=0.3, hatch=\"x\", edgecolor=\"none\",label=\"CrossValidation Set\")\n",
    "    ax1.bar(inds, scores_predictive, color=sns.color_palette(color)[0], label=\"Testing set\")\n",
    "    ax1.set_ylim(0.4, 1)\n",
    "    ax1.set_ylabel(\"Accuracy score\")\n",
    "    ax1.axhline(0.5793, color=\"black\", linestyle=\"--\")\n",
    "    ax1.set_title(\"Accuracy scores for basic models\", fontsize=17)\n",
    "    ax1.set_xticks(range(1,8))\n",
    "    ax1.set_xticklabels(labels, size=12, rotation=40, ha=\"right\")\n",
    "    ax1.legend()\n",
    "\n",
    "    labels = [\"KNN\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\",'Extra Trees', 'AdaBoost', 'SVC' ]\n",
    "    for label, pred in zip(labels, [knn_pred, lg_pred, dt_pred, rf_pred, et_pred, ab_pred, svc_pred]):\n",
    "        fpr, tpr, threshold = roc_curve(y_test.values, pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax2.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=2)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "    ax2.set_xlim([-0.05, 1.0])\n",
    "    ax2.set_ylim([-0.05, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.legend(loc=\"lower right\", prop={'size': 12})\n",
    "    ax2.set_title(\"Roc curve for for basic models\", fontsize=17)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_GridSearch(X_train, X_test, y_train, y_test, kfold):\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "    print(); print(BOLD + 'Grid Search with Random Forest Classifier Model:' + END)\n",
    "    #kfold=5\n",
    "    rf_params = {\n",
    "        #'n_estimators': [10, 50, 100, 150, 200, 250],\n",
    "        'max_features':[2, 3, 5, 7, 8],\n",
    "        #'max_depth': [1, 2, 3, 4, 5, 8],\n",
    "        #'criterion':['gini', 'entropy'],\n",
    "    }\n",
    "\n",
    "    random_forest = RandomForestClassifier(n_estimators=100)\n",
    "    gs = GridSearchCV(random_forest, param_grid=rf_params, cv=5, verbose = 1)\n",
    "    gs_pred, gs_test, gs_train = model_metrics(gs, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    return gs.best_estimator_, gs_pred, gs_test, gs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtraTrees_GridSearch(X_train, X_test, y_train, y_test, kfold):\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "    print(); print(BOLD + 'Grid Search with Extra Trees Model:' + END)\n",
    "    # Scaling \n",
    "      \n",
    "    rf_params = {\n",
    "        #'n_estimators': [10, 100, 400, 800, 1100, 1850],\n",
    "        #'max_features':['auto'],\n",
    "        'max_depth': [1, 2, 3, 4, 5, 8],\n",
    "        #'criterion':['gini'],\n",
    "    }\n",
    "\n",
    "    extra_trees = ExtraTreesClassifier(n_estimators=100)    \n",
    "    gs = GridSearchCV(extra_trees, param_grid=rf_params, cv=5, verbose = 1)\n",
    "    gs_pred, gs_test, gs_train = model_metrics(gs, kfold, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    return gs.best_estimator_, gs_pred, gs_test, gs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_ET_GridSearch (X_train, X_test, y_train, y_test, kfold):\n",
    "    rf_gs_best_estimator, rf_pred, rf_test, rf_train = RandomForest_GridSearch(X_train, X_test, y_train, y_test, kfold)\n",
    "    et_gs_best_estimator, et_pred, et_test, et_train = ExtraTrees_GridSearch(X_train, X_test, y_train, y_test, kfold)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(13,6))\n",
    "\n",
    "    fig.suptitle(\"Random Forest and Extra Trees with Grid Search\", fontsize=16)\n",
    "    #bar chart of accuracy scores\n",
    "    inds = range(1,3)\n",
    "    labels = [\"Random Forest\", \"Extra Trees\" ]\n",
    "    scores_all = [rf_train, et_train]\n",
    "    scores_predictive = [rf_test, et_test]\n",
    "    \n",
    "    ax1.bar(inds, scores_all, color=sns.color_palette(color)[5], alpha=0.3, hatch=\"x\", edgecolor=\"none\",label=\"CrossValidation Set\") #\n",
    "    ax1.bar(inds, scores_predictive, color=sns.color_palette(color)[0], label=\"Testing set\")\n",
    "    ax1.set_ylim(0.4, 1)\n",
    "    ax1.set_ylabel(\"Accuracy score\")\n",
    "    ax1.axhline(0.5793, color=\"black\", linestyle=\"--\")\n",
    "    ax1.set_title(\"Accuracy scores\", fontsize=17)\n",
    "    ax1.set_xticks(range(1,3))\n",
    "    ax1.set_xticklabels(labels, size=14)\n",
    "    ax1.legend()\n",
    "\n",
    "    labels = [\"Random Forest\", \"Extra Trees\" ]\n",
    "    for label, pred in zip(labels, [rf_pred, et_pred]):\n",
    "        fpr, tpr, threshold = roc_curve(y_test.values, pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax2.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=2)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "    ax2.set_xlim([-0.05, 1.0])\n",
    "    ax2.set_ylim([-0.05, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.legend(loc=\"lower right\", prop={'size': 14})\n",
    "    ax2.set_title(\"Roc curve\", fontsize=17)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_visualization (labels, scores_all, scores_predictive, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(13,6))\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    #bar chart of accuracy scores\n",
    "    inds = range(1,3)\n",
    "\n",
    "    ax1.bar(inds, scores_all, color=sns.color_palette(color)[5], alpha=0.3, hatch=\"x\", edgecolor=\"none\",label=\"CrossValidation Set\") #\n",
    "    ax1.bar(inds, scores_predictive, color=sns.color_palette(color)[0], label=\"Testing set\")\n",
    "    ax1.set_ylim(0.4, 1)\n",
    "    ax1.set_ylabel(\"Accuracy score\")\n",
    "    ax1.axhline(0.5793, color=\"black\", linestyle=\"--\")\n",
    "    ax1.set_title(\"Accuracy scores\", fontsize=17)\n",
    "    ax1.set_xticks(range(1,3))\n",
    "    ax1.set_xticklabels(labels, size=14)\n",
    "    ax1.legend()\n",
    "\n",
    "    labels = [\"XGBoost (1)\", \"XGBoost (3)\" ]\n",
    "    for label, pred in zip(labels, [xgb1_pred, xgb2_pred]):\n",
    "        fpr, tpr, threshold = roc_curve(y_test.values, pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax2.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=2)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "    ax2.set_xlim([-0.05, 1.0])\n",
    "    ax2.set_ylim([-0.05, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.legend(loc=\"lower right\", prop={'size': 14})\n",
    "    ax2.set_title(\"Roc curve\", fontsize=17)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
